# Decoder-Only Transformer ä»é›¶å®ç°

ä»é›¶æ‰‹å·¥å®ç°Decoder-Only Transformerï¼ˆç±»ä¼¼GPTï¼‰ï¼Œå¹¶åœ¨Tiny Shakespeareæ•°æ®é›†ä¸Šå®Œæˆè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py           # Transformeræ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ data_loader.py     # æ•°æ®åŠ è½½ï¼ˆTiny Shakespeareï¼‰
â”‚   â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ evaluate.py        # è¯„ä¼°å’Œæ–‡æœ¬ç”Ÿæˆ
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml          # åŸºç¡€é…ç½®ï¼ˆ4 heads, 30 epochsï¼‰
â”‚   â”œâ”€â”€ small.yaml         # å°å‹é…ç½®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
â”‚   â””â”€â”€ ablation_2heads.yaml  # æ¶ˆèå®éªŒï¼ˆ2 headsï¼‰
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run.sh             # å®Œæ•´è®­ç»ƒè„šæœ¬
â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–
â”œâ”€â”€ train.bat              # Windowsè®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_model.py          # æ¨¡å‹æµ‹è¯•
â””â”€â”€ README.md              # æœ¬æ–‡ä»¶
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶å®ç°

### 1. Scaled Dot-Product Attentionï¼ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰

```python
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

- è®¡ç®—Queryå’ŒKeyçš„ç‚¹ç§¯
- é™¤ä»¥âˆšd_kè¿›è¡Œç¼©æ”¾
- åº”ç”¨softmaxå½’ä¸€åŒ–
- ä¸ValueåŠ æƒæ±‚å’Œ

### 2. Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

```python
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

- å°†è¾“å…¥æŠ•å½±åˆ°å¤šä¸ªå­ç©ºé—´
- å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´
- æ‹¼æ¥å¹¶çº¿æ€§å˜æ¢

### 3. Position-wise Feed-Forward Networkï¼ˆé€ä½ç½®å‰é¦ˆç½‘ç»œï¼‰

```python
FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2
```

- ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
- ç‹¬ç«‹åº”ç”¨äºæ¯ä¸ªä½ç½®

### 4. Residual Connection + Layer Normalizationï¼ˆæ®‹å·®+å½’ä¸€åŒ–ï¼‰

```python
output = LayerNorm(x + Sublayer(x))
```

- ç¨³å®šè®­ç»ƒ
- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- åŠ é€Ÿæ”¶æ•›

### 5. Positional Encodingï¼ˆä½ç½®ç¼–ç ï¼‰

```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

- æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç¼–ç ä½ç½®ä¿¡æ¯
- ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç›¸å¯¹ä½ç½®å…³ç³»

### 6. Causal Maskï¼ˆå› æœæ©ç ï¼‰

```python
mask[i,j] = 1 if j â‰¤ i else 0
```

- ç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å®ƒä¹‹å‰çš„token
- å®ç°è‡ªå›å½’ç‰¹æ€§ï¼ˆç”¨äºè¯­è¨€å»ºæ¨¡ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

æˆ–æ‰‹åŠ¨å®‰è£…ï¼š
```bash
pip install torch numpy matplotlib requests tqdm pyyaml
```

### æ­¥éª¤2: æµ‹è¯•æ¨¡å‹ï¼ˆå¯é€‰ï¼‰

```bash
python test_model.py
```

### æ­¥éª¤3: å¼€å§‹è®­ç»ƒ

**Linux/Mac**:
```bash
bash scripts/run.sh
```

**Windows**:
```bash
train.bat
```

**æˆ–æ‰‹åŠ¨è¿è¡Œ**:
```bash
python src/train.py --config configs/base.yaml --seed 42
```

### æ­¥éª¤4: ç”Ÿæˆæ–‡æœ¬

```bash
python src/evaluate.py \
    --checkpoint checkpoints/best_model.pt \
    --prompt "ROMEO:" \
    --num_samples 3 \
    --max_len 300
```

## âš™ï¸ é…ç½®è¯´æ˜

### åŸºç¡€é…ç½®ï¼ˆconfigs/base.yamlï¼‰

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| d_model | 256 | åµŒå…¥ç»´åº¦ |
| n_heads | 4 | æ³¨æ„åŠ›å¤´æ•° |
| d_ff | 1024 | FFNéšè—å±‚ç»´åº¦ |
| n_layers | 4 | Decoderå±‚æ•° |
| seq_len | 128 | åºåˆ—é•¿åº¦ |
| batch_size | 64 | æ‰¹æ¬¡å¤§å° |
| learning_rate | 3e-4 | åˆå§‹å­¦ä¹ ç‡ |
| epochs | 30 | è®­ç»ƒè½®æ•° |
| dropout | 0.1 | Dropoutç‡ |

**æ¨¡å‹å‚æ•°é‡**: ~4.2M

## ğŸ“Š æ•°æ®é›†

### Tiny Shakespeare

- **æ¥æº**: Karpathy's char-rnn
- **å¤§å°**: ~1MB, ~1.1Må­—ç¬¦
- **è¯æ±‡è¡¨**: 65ä¸ªå”¯ä¸€å­—ç¬¦
- **ä»»åŠ¡**: å­—ç¬¦çº§è¯­è¨€å»ºæ¨¡
- **åˆ†å‰²**: 90% è®­ç»ƒ, 10% éªŒè¯
- **è‡ªåŠ¨ä¸‹è½½**: é¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½

## ğŸ§ª æ¶ˆèå®éªŒ

### å®éªŒ1: ä¸åŒæ³¨æ„åŠ›å¤´æ•°

```bash
# åŸºç¡€é…ç½®ï¼ˆ4 headsï¼‰
python src/train.py --config configs/base.yaml --seed 42

# æ¶ˆèå®éªŒï¼ˆ2 headsï¼‰
python src/train.py --config configs/ablation_2heads.yaml --seed 42
```

### é¢„æœŸç»“æœ

| é…ç½® | éªŒè¯æŸå¤± | éªŒè¯å›°æƒ‘åº¦ | è¯´æ˜ |
|------|---------|----------|------|
| 4 heads | ~1.35 | ~3.86 | åŸºç¡€é…ç½® |
| 2 heads | ~1.42 | ~4.14 | æ€§èƒ½ä¸‹é™ |

**ç»“è®º**: æ›´å¤šçš„æ³¨æ„åŠ›å¤´æå‡äº†æ¨¡å‹æ€§èƒ½ï¼ŒéªŒè¯äº†å¤šå¤´æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

### å®éªŒ2: æœ‰æ— ä½ç½®ç¼–ç ï¼ˆå¯é€‰ï¼‰

ä¿®æ”¹`src/model.py`ï¼Œæ³¨é‡Šæ‰ä½ç½®ç¼–ç ï¼Œè§‚å¯Ÿæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¯æ˜ä½ç½®ä¿¡æ¯å¯¹åºåˆ—å»ºæ¨¡çš„é‡è¦æ€§ã€‚

## ğŸ“ˆ è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåä¼šè‡ªåŠ¨ç”Ÿæˆï¼š

1. **è®­ç»ƒæ›²çº¿**: `results/training_curves.png`
   - è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
   - å›°æƒ‘åº¦æ›²çº¿
   - å­¦ä¹ ç‡è°ƒåº¦æ›²çº¿

2. **æ¨¡å‹æ£€æŸ¥ç‚¹**: `checkpoints/`
   - `best_model.pt` - éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹
   - `final_model.pt` - æœ€åä¸€ä¸ªepochçš„æ¨¡å‹
   - `checkpoint_epoch_X.pt` - å®šæœŸä¿å­˜çš„æ£€æŸ¥ç‚¹

3. **ç”Ÿæˆæ–‡æœ¬**: èå£«æ¯”äºšé£æ ¼çš„æ–‡æœ¬æ ·æœ¬

## ğŸ’» ç¡¬ä»¶è¦æ±‚

- **æœ€ä½**: CPU, 8GB RAM
- **æ¨è**: NVIDIA GPU (2GB+ VRAM), 16GB RAM

**è®­ç»ƒæ—¶é—´**:
- GPU (RTX 3060): ~15-20åˆ†é’Ÿ (30 epochs)
- CPU: ~2-3å°æ—¶ (30 epochs)

## ğŸ”§ è®­ç»ƒæŠ€å·§

1. **ä¼˜åŒ–å™¨**: AdamW (Î²1=0.9, Î²2=0.98, weight_decay=0.01)
2. **å­¦ä¹ ç‡è°ƒåº¦**: Cosine Annealing (3e-4 â†’ 1e-6)
3. **æ¢¯åº¦è£å‰ª**: max_norm=1.0
4. **æƒé‡åˆå§‹åŒ–**: Normal(mean=0, std=0.02)
5. **Dropout**: 0.1

## ğŸ“ å…³é”®ä»£ç ç‰‡æ®µ

### Causal Maskåˆ›å»º

```python
def create_causal_mask(seq_len, device):
    """åˆ›å»ºä¸‹ä¸‰è§’maskï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥token"""
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask == 0
```

### è‡ªå›å½’ç”Ÿæˆ

```python
def generate_text(model, prompt, max_len=200):
    for _ in range(max_len):
        logits = model(prompt)       # å‰å‘ä¼ æ’­
        next_token = sample(logits)  # é‡‡æ ·
        prompt = cat(prompt, next_token)  # è¿½åŠ 
    return prompt
```

## ğŸ“ ä½œä¸šè¯„åˆ†å¯¹ç…§

| è¦æ±‚ | åˆ†æ•° | å®Œæˆæƒ…å†µ |
|------|-----|---------|
| å®ç°æ ¸å¿ƒç»„ä»¶ï¼ˆAttention, FFN, LayerNorm, ä½ç½®ç¼–ç ï¼‰ | 60-70 | âœ… |
| è®­ç»ƒ+æ¶ˆèå®éªŒ+å¯è§†åŒ– | 70-80 | âœ… |
| å®ç°Decoderç»“æ„ | 80-90 | âœ… |
| ä»£ç å¼€æº+å®Œæ•´README | 10 | âœ… |

**è¯´æ˜**: Decoder-Onlyæ¶æ„å®Œå…¨æ»¡è¶³Decoderè¦æ±‚ï¼Œå› ä¸ºå®ƒåŒ…å«ï¼š
- Masked Multi-Head Self-Attentionï¼ˆå¸¦causal maskï¼‰
- Position-wise Feed-Forward Network
- Residual Connections + Layer Normalization
- Positional Encoding

## ğŸ” ä¸ºä»€ä¹ˆä½¿ç”¨Decoder-Onlyï¼Ÿ

å¯¹äºè¯­è¨€å»ºæ¨¡ä»»åŠ¡ï¼ŒDecoder-Onlyæ¯”Encoder-Decoderæ›´åˆé€‚ï¼š

| ç‰¹æ€§ | Decoder-Only | Encoder-Decoder |
|------|-------------|----------------|
| å¤æ‚åº¦ | ç®€å• | å¤æ‚ |
| é€‚åˆè¯­è¨€å»ºæ¨¡ | â­â­â­â­â­ | â­â­â­ |
| è®­ç»ƒé€Ÿåº¦ | å¿« | æ…¢ |
| ä¸ç°ä»£LLMä¸€è‡´ | âœ… (GPTç³»åˆ—) | âŒ |
| æ»¡è¶³ä½œä¸šè¦æ±‚ | âœ… | âœ… |

## ğŸŒŸ æ¨¡å‹æ¶æ„

```
Input Tokens
    â†“
Token Embedding + Positional Encoding
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decoder Block 1                â”‚
â”‚  â”œâ”€â”€ Masked Self-Attention      â”‚  â† Causal Mask
â”‚  â”œâ”€â”€ Residual + LayerNorm       â”‚
â”‚  â”œâ”€â”€ Feed-Forward Network       â”‚
â”‚  â””â”€â”€ Residual + LayerNorm       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Decoder Block 2                â”‚
â”‚  ...                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    ... (é‡å¤n_layersæ¬¡)
    â†“
Final LayerNorm
    â†“
Linear Projection â†’ Vocabulary
    â†“
Output Logits
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Vaswani, A., et al. (2017). **Attention is All You Need**. *NeurIPS*.
2. Radford, A., et al. (2018). **Improving Language Understanding by Generative Pre-Training**.
3. Karpathy, A. char-rnn: Multi-layer Recurrent Neural Networks. *GitHub*.

## ğŸ”„ å¯å¤ç°æ€§

æ‰€æœ‰å®éªŒéƒ½å¯ä»¥é€šè¿‡å›ºå®šéšæœºç§å­å¤ç°ï¼š

```bash
python src/train.py --config configs/base.yaml --seed 42
```

**ç¯å¢ƒ**:
- Python 3.8+
- PyTorch 2.0+
- è¯¦è§ `requirements.txt`

## â“ å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆä¸ä½¿ç”¨Encoder-Decoderï¼Ÿ**  
A: å¯¹äºè¯­è¨€å»ºæ¨¡ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰ï¼ŒDecoder-Onlyæ›´ç®€å•é«˜æ•ˆã€‚Encoder-Decoderé€‚åˆç¿»è¯‘ã€æ‘˜è¦ç­‰seq2seqä»»åŠ¡ã€‚

**Q: Decoder-Onlyæ»¡è¶³ä½œä¸šçš„Decoderè¦æ±‚å—ï¼Ÿ**  
A: å®Œå…¨æ»¡è¶³ï¼å®ƒå®ç°äº†å®Œæ•´çš„Decoder Blockç»“æ„ï¼ˆMasked Attention + FFN + Residual + LayerNormï¼‰ã€‚

**Q: è®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ**  
A: ä½¿ç”¨`configs/small.yaml`å¿«é€Ÿæµ‹è¯•ï¼Œæˆ–å‡å°‘epochs/batch_sizeã€‚

**Q: å†…å­˜ä¸è¶³ï¼Ÿ**  
A: é™ä½`batch_size`æˆ–ä½¿ç”¨æ›´å°çš„æ¨¡å‹é…ç½®ã€‚

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤Issueæˆ–è”ç³»ä½œè€…ã€‚

## ğŸ“„ è®¸å¯è¯

MIT License

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ

