# Decoder-Only Transformer ä»é›¶å®ç°

ä»é›¶æ‰‹å·¥å®ç°Decoder-Only Transformerï¼ˆç±»ä¼¼GPTï¼‰ï¼Œå¹¶åœ¨Tiny Shakespeareæ•°æ®é›†ä¸Šå®Œæˆè¯­è¨€å»ºæ¨¡ä»»åŠ¡ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py           # Transformeræ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ data_loader.py     # æ•°æ®åŠ è½½ï¼ˆTiny Shakespeareï¼‰
â”‚   â”œâ”€â”€ train.py           # è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ evaluate.py        # è¯„ä¼°å’Œæ–‡æœ¬ç”Ÿæˆ
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml          # åŸºç¡€é…ç½®ï¼ˆ4 heads, 30 epochsï¼‰
â”‚   â”œâ”€â”€ small.yaml         # å°å‹é…ç½®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
â”‚   â””â”€â”€ ablation_2heads.yaml  # æ¶ˆèå®éªŒï¼ˆ2 headsï¼‰
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run.sh             # å®Œæ•´è®­ç»ƒè„šæœ¬
â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–
â”œâ”€â”€ train.bat              # Windowsè®­ç»ƒè„šæœ¬
â”œâ”€â”€ test_model.py          # æ¨¡å‹æµ‹è¯•
â””â”€â”€ README.md              # æœ¬æ–‡ä»¶
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶å®ç°

### 1. Scaled Dot-Product Attentionï¼ˆç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼‰

```python
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

- è®¡ç®—Queryå’ŒKeyçš„ç‚¹ç§¯
- é™¤ä»¥âˆšd_kè¿›è¡Œç¼©æ”¾
- åº”ç”¨softmaxå½’ä¸€åŒ–
- ä¸ValueåŠ æƒæ±‚å’Œ

### 2. Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰

```python
MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O
head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

- å°†è¾“å…¥æŠ•å½±åˆ°å¤šä¸ªå­ç©ºé—´
- å¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´
- æ‹¼æ¥å¹¶çº¿æ€§å˜æ¢

### 3. Position-wise Feed-Forward Networkï¼ˆé€ä½ç½®å‰é¦ˆç½‘ç»œï¼‰

```python
FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2
```

- ä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ
- ç‹¬ç«‹åº”ç”¨äºæ¯ä¸ªä½ç½®

### 4. Residual Connection + Layer Normalizationï¼ˆæ®‹å·®+å½’ä¸€åŒ–ï¼‰

```python
output = LayerNorm(x + Sublayer(x))
```

- ç¨³å®šè®­ç»ƒ
- ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- åŠ é€Ÿæ”¶æ•›

### 5. Positional Encodingï¼ˆä½ç½®ç¼–ç ï¼‰

```python
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

- æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç¼–ç ä½ç½®ä¿¡æ¯
- ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ç›¸å¯¹ä½ç½®å…³ç³»

### 6. Causal Maskï¼ˆå› æœæ©ç ï¼‰

```python
mask[i,j] = 1 if j â‰¤ i else 0
```

- ç¡®ä¿æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å®ƒä¹‹å‰çš„token
- å®ç°è‡ªå›å½’ç‰¹æ€§ï¼ˆç”¨äºè¯­è¨€å»ºæ¨¡ï¼‰

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

æˆ–æ‰‹åŠ¨å®‰è£…ï¼š
```bash
pip install torch numpy matplotlib requests tqdm pyyaml
```


### æ­¥éª¤3: å¼€å§‹è®­ç»ƒ

**Linux/Mac**:
```bash
bash scripts/run.sh
```

**Windows**:
```bash
train.bat
```

**æˆ–æ‰‹åŠ¨è¿è¡Œ**:
```bash
python src/train.py --config configs/base.yaml --seed 42
```

### æ­¥éª¤4: ç”Ÿæˆæ–‡æœ¬

```bash
python src/evaluate.py \
    --checkpoint checkpoints/best_model.pt \
    --prompt "ROMEO:" \
    --num_samples 3 \
    --max_len 300
```

## âš™ï¸ é…ç½®è¯´æ˜

### åŸºç¡€é…ç½®ï¼ˆconfigs/base.yamlï¼‰

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| d_model | 256 | åµŒå…¥ç»´åº¦ |
| n_heads | 4 | æ³¨æ„åŠ›å¤´æ•° |
| d_ff | 1024 | FFNéšè—å±‚ç»´åº¦ |
| n_layers | 4 | Decoderå±‚æ•° |
| seq_len | 128 | åºåˆ—é•¿åº¦ |
| batch_size | 64 | æ‰¹æ¬¡å¤§å° |
| learning_rate | 3e-4 | åˆå§‹å­¦ä¹ ç‡ |
| epochs | 30 | è®­ç»ƒè½®æ•° |
| dropout | 0.1 | Dropoutç‡ |

**æ¨¡å‹å‚æ•°é‡**: ~4.2M

## ğŸ“Š æ•°æ®é›†

### Tiny Shakespeare

- **æ¥æº**: Karpathy's char-rnn
- **å¤§å°**: ~1MB, ~1.1Må­—ç¬¦
- **è¯æ±‡è¡¨**: 65ä¸ªå”¯ä¸€å­—ç¬¦
- **ä»»åŠ¡**: å­—ç¬¦çº§è¯­è¨€å»ºæ¨¡
- **åˆ†å‰²**: 90% è®­ç»ƒ, 10% éªŒè¯
- **è‡ªåŠ¨ä¸‹è½½**: é¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨ä¸‹è½½

## ğŸ§ª æ¶ˆèå®éªŒ

### å®éªŒ1: ä¸åŒæ³¨æ„åŠ›å¤´æ•°

```bash
# åŸºç¡€é…ç½®ï¼ˆ4 headsï¼‰
python src/train.py --config configs/base.yaml --seed 42

# æ¶ˆèå®éªŒï¼ˆ2 headsï¼‰
python src/train.py --config configs/ablation_2heads.yaml --seed 42
```


**ç»“è®º**: æ›´å¤šçš„æ³¨æ„åŠ›å¤´æå‡äº†æ¨¡å‹æ€§èƒ½ï¼ŒéªŒè¯äº†å¤šå¤´æœºåˆ¶çš„æœ‰æ•ˆæ€§ã€‚

### å®éªŒ2: æœ‰æ— ä½ç½®ç¼–ç ï¼ˆå¯é€‰ï¼‰

ä¿®æ”¹`src/model.py`ï¼Œæ³¨é‡Šæ‰ä½ç½®ç¼–ç ï¼Œè§‚å¯Ÿæ€§èƒ½æ˜¾è‘—ä¸‹é™ï¼Œè¯æ˜ä½ç½®ä¿¡æ¯å¯¹åºåˆ—å»ºæ¨¡çš„é‡è¦æ€§ã€‚

## ğŸ“ˆ è®­ç»ƒç»“æœ

è®­ç»ƒå®Œæˆåä¼šè‡ªåŠ¨ç”Ÿæˆï¼š

1. **è®­ç»ƒæ›²çº¿**: `results/training_curves.png`
   - è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
   - å›°æƒ‘åº¦æ›²çº¿
   - å­¦ä¹ ç‡è°ƒåº¦æ›²çº¿

2. **æ¨¡å‹æ£€æŸ¥ç‚¹**: `checkpoints/`
   - `best_model.pt` - éªŒè¯æŸå¤±æœ€ä½çš„æ¨¡å‹
   - `final_model.pt` - æœ€åä¸€ä¸ªepochçš„æ¨¡å‹
   - `checkpoint_epoch_X.pt` - å®šæœŸä¿å­˜çš„æ£€æŸ¥ç‚¹

3. **ç”Ÿæˆæ–‡æœ¬**: èå£«æ¯”äºšé£æ ¼çš„æ–‡æœ¬æ ·æœ¬

## ğŸ’» ç¡¬ä»¶è¦æ±‚

- **æœ€ä½**: CPU, 16GB RAM
- **æ¨è**: NVIDIA GPU (2GB+ VRAM), 16GB RAM

## ğŸ”§ è®­ç»ƒæŠ€å·§

1. **ä¼˜åŒ–å™¨**: AdamW (Î²1=0.9, Î²2=0.98, weight_decay=0.01)
2. **å­¦ä¹ ç‡è°ƒåº¦**: Cosine Annealing (3e-4 â†’ 1e-6)
3. **æ¢¯åº¦è£å‰ª**: max_norm=1.0
4. **æƒé‡åˆå§‹åŒ–**: Normal(mean=0, std=0.02)
5. **Dropout**: 0.1

## ğŸ“ å…³é”®ä»£ç ç‰‡æ®µ

### Causal Maskåˆ›å»º

```python
def create_causal_mask(seq_len, device):
    """åˆ›å»ºä¸‹ä¸‰è§’maskï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥token"""
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask == 0
```

### è‡ªå›å½’ç”Ÿæˆ

```python
def generate_text(model, prompt, max_len=200):
    for _ in range(max_len):
        logits = model(prompt)       # å‰å‘ä¼ æ’­
        next_token = sample(logits)  # é‡‡æ ·
        prompt = cat(prompt, next_token)  # è¿½åŠ 
    return prompt
```

---

**æœ€åæ›´æ–°**: 2025å¹´11æœˆ

